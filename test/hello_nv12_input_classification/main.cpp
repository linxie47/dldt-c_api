// Copyright (C) 2019 Intel Corporation
// SPDX-License-Identifier: Apache-2.0
//

#include <fstream>
#include <iostream>
#include <memory>
#include <sstream>
#include <string>
#include <utility>
#include <vector>

#include <ie_compound_blob.h>
#include <inference_engine.hpp>

#include "classification_results.h"
#include "ie_c_api.h"
#include "ie_api_impl.hpp"

extern "C" {
void *infer_request_get_object(infer_request_t *infer_request);
}

const char *CONFIGS = "CPU_THREADS_NUM=4|CPU_THROUGHPUT_STREAMS=4|CPU_BIND_THREAD=NO";

using namespace InferenceEngine;

namespace IEPY = InferenceEnginePython;
namespace IE = InferenceEngine;

/**
 * \brief Parse image size provided as string in format WIDTHxHEIGHT
 * @return parsed width and height
 */
std::pair<size_t, size_t> parseImageSize(const std::string &size_string) {
    auto delimiter_pos = size_string.find("x");
    if (delimiter_pos == std::string::npos || delimiter_pos >= size_string.size() - 1 || delimiter_pos == 0) {
        std::stringstream err;
        err << "Incorrect format of image size parameter, expected WIDTHxHEIGHT, "
               "actual: "
            << size_string;
        throw std::runtime_error(err.str());
    }

    size_t width = static_cast<size_t>(std::stoull(size_string.substr(0, delimiter_pos)));
    size_t height = static_cast<size_t>(std::stoull(size_string.substr(delimiter_pos + 1, size_string.size())));

    if (width == 0 || height == 0) {
        throw std::runtime_error("Incorrect format of image size parameter, width and height must not be equal to 0");
    }

    if (width % 2 != 0 || height % 2 != 0) {
        throw std::runtime_error("Unsupported image size, width and height must be even numbers");
    }

    return {width, height};
}

/**
 * \brief Read image data from file
 * @return buffer containing the image data
 */
std::unique_ptr<unsigned char[]> readImageDataFromFile(const std::string &image_path, size_t size) {
    std::ifstream file(image_path, std::ios_base::ate | std::ios_base::binary);
    if (!file.good() || !file.is_open()) {
        std::stringstream err;
        err << "Cannot access input image file. File path: " << image_path;
        throw std::runtime_error(err.str());
    }

    const size_t file_size = file.tellg();
    if (file_size < size) {
        std::stringstream err;
        err << "Invalid read size provided. File size: " << file_size << ", to read: " << size;
        throw std::runtime_error(err.str());
    }
    file.seekg(0);

    std::unique_ptr<unsigned char[]> data(new unsigned char[size]);
    file.read(reinterpret_cast<char *>(data.get()), size);
    return data;
}

/**
 * \brief Sets batch size of the network to the specified value
 */
void setBatchSize(CNNNetwork &network, size_t batch) {
    ICNNNetwork::InputShapes inputShapes = network.getInputShapes();
    for (auto &shape : inputShapes) {
        auto &dims = shape.second;
        if (dims.empty()) {
            throw std::runtime_error("Network's input shapes have empty dimensions");
        }
        dims[0] = batch;
    }
    network.reshape(inputShapes);
}

/**
 * @brief The entry point of the Inference Engine sample application
 */
int main(int argc, char *argv[]) {
    try {
        // ------------------------------ Parsing and validatiing input arguments------------------------------
        if (argc != 5) {
            std::cout << "Usage : ./hello_nv12_input_classification <path_to_model> <path_to_image> <image_size> "
                         "<device_name>"
                      << std::endl;
            return EXIT_FAILURE;
        }

        const std::string input_model{argv[1]};
        const std::string input_image_path{argv[2]};
        size_t input_width = 0, input_height = 0;
        std::tie(input_width, input_height) = parseImageSize(argv[3]);
        const std::string device_name{argv[4]};
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 1. Load inference engine ------------------------------------------------
        // Core ie;
        ie_core_t *core = ie_core_create();
        if (device_name.compare("CPU") == 0) {
            ie_core_set_config(core, CONFIGS, argv[4]);
            ie_core_add_extension(core, NULL, "CPU");
        }
        // -----------------------------------------------------------------------------------------------------

        // -------------------------- 2. Read the IR generated by the Model Optimizer (.xml and .bin files) ----
        ie_network_t *ie_network = ie_network_create(core, input_model.c_str(), NULL);
        ie_network_set_batch(ie_network, 1);
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 3. Configure input and output -------------------------------------------
        // --------------------------- Prepare input blobs -----------------------------------------------------
        if (ie_network_get_input_number(ie_network) == 0) {
            std::cerr << "Network inputs info is empty" << std::endl;
            return EXIT_FAILURE;
        }

        ie_input_info_t input_info = {};
        // get the 1st input
        ie_network_get_input(ie_network, &input_info, NULL);
        ie_input_info_set_precision(&input_info, "U8");
        ie_input_info_set_layout(&input_info, "NCHW");

        ie_input_info_set_preprocess(&input_info, ie_network, IEResizeAlg::RESIZE_BILINEAR, IEColorFormat::NV12);

        // --------------------------- Prepare output blobs ----------------------------------------------------
        if (ie_network_get_output_number(ie_network) == 0) {
            std::cerr << "Network outputs info is empty" << std::endl;
            return EXIT_FAILURE;
        }

        ie_output_info_t output_info;
        // get the 1st output
        ie_network_get_output(ie_network, &output_info, NULL);
        ie_output_info_set_precision(&output_info, "FP32");
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 4. Loading a model to the device ----------------------------------------
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 5. Create an infer request ----------------------------------------------
        infer_requests_t *infer_requests = ie_network_create_infer_requests(ie_network, 1, device_name.c_str());
        infer_request_t *infer_req = infer_requests->requests[0];
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 6. Prepare input --------------------------------------------------------
        // read image with size converted to NV12 data size: height(NV12) = 3 / 2 * logical height
        auto image_buf = readImageDataFromFile(input_image_path, input_width * (input_height * 3 / 2));
        const size_t offset = input_width * input_height;

        #if 0
        // --------------------------- Create a blob to hold the NV12 input data -------------------------------
        // Create tensor descriptors for Y and UV blobs
        InferenceEngine::TensorDesc y_plane_desc(InferenceEngine::Precision::U8,
            {1, 1, input_height, input_width}, InferenceEngine::Layout::NHWC);
        InferenceEngine::TensorDesc uv_plane_desc(InferenceEngine::Precision::U8,
            {1, 2, input_height / 2, input_width / 2}, InferenceEngine::Layout::NHWC);

        // Create blob for Y plane from raw data
        Blob::Ptr y_blob = make_shared_blob<uint8_t>(y_plane_desc, image_buf.get());
        // Create blob for UV plane from raw data
        Blob::Ptr uv_blob = make_shared_blob<uint8_t>(uv_plane_desc, image_buf.get() + offset);
        // Create NV12Blob from Y and UV blobs
        Blob::Ptr input = make_shared_blob<NV12Blob>(y_blob, uv_blob);

        {
            void *object = infer_request_get_object(infer_req);

            InferenceEnginePython::InferRequestWrap *infer_wrap =
                reinterpret_cast<InferenceEnginePython::InferRequestWrap *>(object);

            InferenceEngine::ResponseDesc res = {};
            const bool compoundBlobPassed = input->is<CompoundBlob>();
            std::cout << "input: " << input_info.name << std::endl;
            infer_wrap->request_ptr->SetBlob(input_info.name, input, &res);
            std::cout << "is comp:" << compoundBlobPassed  << " " << res.msg << std::endl;
        }
        #endif
        // --------------------------- Set the input blob to the InferRequest ----------------------------------
        uint8_t *data[] = { image_buf.get(), image_buf.get() + offset };
        infer_request_set_blob(infer_req, input_info.name, input_width, input_height, IEColorFormat::NV12, data, NULL);
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 7. Do inference ---------------------------------------------------------
        /* Running the request synchronously */
        infer_request_infer(infer_req);
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 8. Process output -------------------------------------------------------
        InferenceEngine::Blob::Ptr output;
        void *object = infer_request_get_object(infer_req);
        InferenceEnginePython::InferRequestWrap *infer_wrap =
            reinterpret_cast<InferenceEnginePython::InferRequestWrap *>(object);
        // infer_wrap->getBlobPtr(output_info.name, output);
        try {
            infer_wrap->getBlobPtr(output_info.name, output);
        } catch (const std::exception &e) {
            std::throw_with_nested(std::runtime_error("Can not get blob: " + std::string(output_info.name)));
        }

        // Print classification results
        ClassificationResult classificationResult(output, {input_image_path});

        std::vector<unsigned> results;
        TopResults(10, *output, results);
        classificationResult.print();
        // -----------------------------------------------------------------------------------------------------
    } catch (const std::exception &ex) {
        std::cerr << ex.what() << std::endl;
        return EXIT_FAILURE;
    }
    std::cout << "This sample is an API example, for any performance measurements "
                 "please use the dedicated benchmark_app tool"
              << std::endl;
    return EXIT_SUCCESS;
}
